# SONY柏木さん
## 会社紹介
- 量よりも高付加価値を狙う会社戦略
- aiboにも認識や学習技術が使われている
- 高い技術を醸成する仕組み
- R&Dからも、商品サービス企画部に異動したりすることもできる
- 社内募集制度での異動
- クリエイティビティとテクノロジーで世界を感動で満たす

## Interspeech2019について
- 音声処理に関する学会
- 今年はオーストリア、４日間
- 採択率49%
- 9つものワークショップ
---
# The USTC System for BC2019
## 小谷さん（東大齋藤研究室D1）
- 声質変換
- TTSもしたい

## 論文概要
- 中国語TTSのタスクが一番良かったシステムを紹介

## Blizzard Challenge 2019
- ExpressiveなTTSを作れ！というタスク
- ブレスや抑揚の影響で変換前がそもそもExpressiveである

## FW
- BERTベースのフロントエンド

### 前処理
- 人手の処理のコスト
- どの語を強調するかも人手で設定
- Mel-cep, F0, V/UV
- HMM alignmentもした

## フロントエンド
- テキスト処理
  - アラビア数字など特殊な文字を変換
- 中国語のバウンダリー予測
- Focus wordsの予測も

### BERT
- 外部データとして小説とニュース記事
- BERTは100GBのデータを使っていた（補足）
- 出力が各識別タスクを解く。BERT本体のところはFIXなのかパラメタ更新しているのかどうか不明
- 計算資源はどうしている？（坂東さん）->企業と協力して資源を使わせてもらっているよう。潤沢。
- (坂東さん、質疑時の余談)NASなんかを産総研のABCI使って回している例はあったりするよう

## Duration Modeling
- 継続長をLogスケールで量子化してクロスエントロピーで学習
- Contextの入力：ピンインと四声
- Pre-trainedのマルチスピーカーモデルを1000時間以上使っていた

## Acoustic Modeling
- GANを使ってOversmoothingを回避している

## Vocoding
- 多人数話者でWavenet vocoder
- 16kHzと24kHzのWavenetをConcatした
- 位相はノイズで埋めてGriffin-Lim的なことをしているらしい

## まとめ
- BC2019
  - BERT-based
  - Autoregressive
  - GAN-based multi-task acoustic model
  - Wavenet-based neural vocoder
- まるで本人のような声が出ていて感動
---
# GMMNに基づく音声合成におけるグラム行列のスパース近似
## 須田さん 東大齋藤研究室D1
## SSW10について
- Neural voocoder / VC など
## 論文について
### 背景
- 一期一会音声合成
- 同じ発話にも揺らぎをもたせたい
- 1次から無限次元までのモーメントマッチングをする

### 提案法
- 平均だけ普通のNNで推定し、分散だけ上側のGMMNで学習する
- CMMDという学習基準をどう速く計算するかがポイント

### 生成的モーメントマッチングネットワーク
- 最大平均差異(MMD)を使う: 分布間距離
- 分布が同一ならMMDが0になる
- MMDの2乗を展開するとグラム行列が出てくる

### 条件付き生成的モーメントマッチングネットワーク
- 条件と観測のグラムベクトルが両方出てくる
- 観測と観測の類似度行列の逆を求めるのが遅い（毎エポック）ので、近似

### 近似式
- ミニバッチ単位で計算する：バッチ間での独立性を仮定して計算コストを下げる
- 乱択化フーリエ特徴（RFF）：縦長と横長の積に分解できる：IFFTは時間単位の期待値だと思う
- O(バッチサイズの３乗)　から O(N*バッチサイズの２乗)へ減らす

## 主観評価実験
- 自然性MOSで優れている
- 発話の違う感MOS：結局継続長だけ変えれば違う印象になる説があるらしい…

## まとめ
- RFFを使ってk-meansでミニバッチ分割して発話内変動のある音声合成ができた
- 聴覚的には継続長の影響が大きかった
- Future Work
  - VAE, GANとの比較
- 感想
  - モーメントマッチングネットワーク最高
  
## QA
- 自然音声での揺らぎは？
  - 継続長は自然音声でも結構ゆれている
  - 単純な２乗誤差だと自然音声の20%くらいしかゆれなかった
- バッチ分割のランダムさとクラスタリングしたものがあったが、このテクニックはオリジナルなのか？
  - k-meansでミニバッチに分割するものはすでにあったと思う
  - Gaussian ProcessのVCではよくやられているらしい
---
# Parrotron: End-to-end speech-to-speech Conversion Model and its applications to hearing-
## 橘さん(DeNA)
- 東芝R&Dで音声合成->NICT->DeNA

## 論文について
- 音声から音声へを波形直接変換
- Many-to-ne音声変換を高品質なレベルで実現
- 従来の音声変換以外にも他の用途に使える

## タスク
- Many-to-one音声変換
- 聴覚障碍者の音声変換
- ノイズ除去や音源分離

## 音声変換
- 包絡、F0, 非周期性指標を変換先の話者のパラメタにへんかnする
### 技術課題
- パラメタ変換が限定的：モデルを学習・変換する時に、
  - F0はオフセットやDynamic rangeを変えるだけだったりする 
  - 非周期性指標はそのままだったりする
- Alignment問題
  - 従来はDTWが直接品質に影響してしまう
- Many-to-one対応が高コスト
  - 多くの話者間のペアを学習しないといけない
  
### 提案法
#### WavenetをはじめとしてNeural Vocoderが登場
- Mel-specから直接波形生成することが可能になった
#### Seq2seqにより系列同士の学習が可能になった
- Attentionのやり方がAdditiveやSCENTなどがある
- ParrotronでもEncoderに対してASRのマルチタスク学習をさせている
#### 大規模書き起こし音声コーパスにTTS適用
- 変換先話者をParallel wavenetで生成
- 大規模音声コーパスが使える：ペアが求まる

### アルゴリズム
- メルスペクトログラムをWavenet vocoderに通す
- Conv2D -> LSTM4層
- DecoderはTacotron2のdecoderと同一構造
- ASR Decoder: Attention Layerの出力と１時刻前の音素予測結果を結合している->マルチタスクによりEncoderがPhonemeを保持する狙い

### 実験
- データセット
- 客観評価
  - ASRのWERで評価。7ポイントの性能改善。17.6%
  - Challengingな音声で評価してもMOSが4くらいになっている
- Many-to-one音声変換
  - 音声変換
  - 聴覚障碍者の15.4時間の音声で学習->変換することでクリアな音声に！
  - 背景ノイズの作成->音源分離タスクを解かせることで33.2->17.3%までWERが低減される
  
### まとめ
- End-to-and 音声変換ができた
- ASR Multitask 学習が有効
- 障碍者音声の明瞭化やノイズ除去という様々なタスクで有効性を確認
---
# Wav2vec: Unsupervised Pre-training for Speech Recognition

## 柏木さん ソニーR&D　音声認識の雑音抑圧などを担当

### 論文紹介

- Wav2vecの目的：人手の特徴量抽出なしで完全なEnd-to-end ASRをやりたい
- ラベルのないデータを利用できないか、というアプローチ
- 古くはRBM、AutoEncoderに似た考え方

#### Pretraining

- 教師なしアプローチにより良いEmbedding空間を構築する
  - 良い＝実験的に探索している（現状は）
- Word2vec, BERT, ViLBERT、Speech2vec, Representation Learning with Contrastive Predictive Coding
  - Speech2vec: 単語の意味に応じた空間を張ってくれる
- BERTの影響は凄まじく、ASRにおいても重要となる技術

#### CPC(Contrastive Predictive Coding)

- 今までMFCCでやってた予測モデルがもっとうまくいったモデルなので
- Encoder networkとEmbedding vectorを構造として持っている
  - Encoder->１次元畳み込みネットワーク
- 目的関数：
  - 1項目は未来のEmbeddingとの相関を取るコスト関数
  - 2項目は負例との相関が低いことを保証するコスト関数
  - 未来何フレームまで予測するかはハイパラ
- ASRでの利用
  - Filter bankなどの特徴の代わりにまるっと置き換える

#### 性能比較

- WSJコーパスの一般的なASR実験：ちゃんとデータを増やすと性能が上がっていく
- ラベル付きデータの差が少ないところで特に顕著に優れた性能が出せる
- 1000時間くらい学習させればPERで12~14％くらいまで下げることができる

### 続報：VQ-wav2vec

- 量子化されたものをBERTに突っ込む
- 10msのEmbeddingを推定することは簡単なため、マスクするフレームは複数にする
- 特にTIMITでBERTいれた瞬間に一気に3ポイントくらい性能が良くなったりした

### まとめ

- PretrainingアプローチのASRの応用
- Wav2vec: 教師なしデータをうまく利用して少量ラベルデータで高い性能を実現
- VQ-wav2vec: BERTの生成モデリングは非常に強力でありASRで重要になると思料
- 一方、手探り感が拭えないのと、マスク系の話でSpecAugmentなども巻き込みながら発展していくのではと予想している

### QA

- 他のBERTの例は？
  - VideoとNLPを使ったマルチモーダルCVみたいな論文もある
- 今回のNLPはテキストデータは全然関係なくて、未来の音声を予測するのみ
- ビットレートに対するPERの比較のグラフは面白かった（圧縮率とは別の話なので注意）
---
# End-to-end Adaptation 



## 角尾さん (SONYで音楽や音声を担当)

### 目指すもの

- ロボむけ組み込みコマンド認識
- 適応データでのモデル再学習
- 認識器全体をEnd-to-endで適応させたい

### 提案手法

- 従来：DNN-HMM: SP -> Feature Extraction -> AM -> LM
  - それぞれが部分ごとに最適化されてしまっている
  - WFSTでは、有限オートマトンのグラフを使ってモデル化する
    - 各時間にどの事後確率が最も高かかったかを計算し、一番可能性のある最適経路探索をする
- 提案法：ViterbiNet: WFSTと等価になるようなモデルを作っている
  - 既存のモデルを初期値としてEnd-to-end学習を可能にしている
  - ViterbiNetでは、グラフ遷移を行列で表現し、再帰的な前向き演算をする
    - 語彙ごとに事後確率を計算してやって、マックスプーリングにより識別問題へを転化させることができる
    - BackPropは順番に後ろ方向→前方向へと計算していく

### 実験

- EM: 5レイヤー640ユニットFCDNN
- English: 28000h, Japanese: 12,300h
- LM: Grammar WFST for eahch task
- 「おいで」や「前進」などの語彙を学習させる
- 結果
  - 従来法よりも良いError Rateに
  - 適応方法：LMは人手でHMM似合うように再設計
    - 例：Seven -> 日本語用にSebnにしてやるなど

### まとめ

- FW: 第5位連続音声認識のための大規模なWFST再学習に取り組み中

### QA

- 間違いにくい語は実験データにあるか？
  - 例えばStart、Stopなどややこしい単語もある
- メモリ使用量は工夫はしている？
  - 学習の時はかなり使っているが、実行時はWFSTの形に直して元と同じサイズにしている
- Sumをとって使うときだけViterbiみたいな使い方はできる？
  - 事前実験ではMax使ったほうがよさそうだったのでそちらを採用した
---
# SpecAugment

## Deepmind

## 篠原さん、PR,CVからASR(AM)へ

## 論文概要

- シンプルなデータ拡張方法を提案
  - スペクトログラム上で一部の周波数、時間をマスキングするだけ
- 複数のASRタスクでSOTA
- 最も注目された論文の1つ、広く利用されつつあり今後標準になるかもしれない

## 背景

- 深層学習によるASRが盛ん、性能が飛躍的に向上、最近ではEnd-to-endのLASなどがある
- しかし、学習データにOverfitしやすいので大規模な学習データが必要であった

## 従来手法

- データ拡張による学習データ増量がASR精度の向上に寄与
  - Kanda+2013
  - Jaitly+2013
  - Kim+2017
- ほとんどは雑音や残響を重畳するなど、音声波形を直接加工するものであり、計算が重かったりする

## 提案法：SpecAugment

- 波形ではなくスペクトトログラムを加工するもの
- Mel-specを直接加工
  - 時間方法への伸縮
    - 入力されたスペクトログラムから適当な量だけ左に行くつかフレームをずらし、全体をWarpingする
  - 周波数情報へのマスキング
    - 連続するf個を全て潰す
  - 時間マスキング
    - 連続するtフレームを無作為にマスキング
- データ加工のポリシー（マスクの最大値）は人手で設計した。計2x2の4種類
  - 時間方向と周波数方向を２箇所ずつ
  - MildとStrong(より広く潰す)の2種類

### ASRモデル：LAS

- Attention付きのSeq2seqモデル
- 出力はWord-piece modelを使用している
- 言語モデルとのShallow fusionも試したりしている

### 学習率スケジュール

- 80k, 160k, 320kステップという3パターンを試している

## 実験

- LibriSpeechとSwitchboardでWERを評価
- Augmentatioありの方が5ポイントくらい減っている
- 学習率のスケジュールは長めの方がいい
- LibriSpeechでSOTA(6%を切った)
- LMなし、Shallow fusionなしでもSOTAになっている
- 時間伸縮はなくてもよさそう、周波数マスクが効いているっぽい
- Underfittingになっている：大きなモデルはもっとゆっくり大きなステップで学習することが大事らしい

## まとめ

- データ拡張によりASR性能が大幅向上
- LASによるend-to-endASRにおいてシンプルなデータ拡張を用いてHMM-hybridを含む従来のシステムを性能を上回った。LibriSpeech, SwitchboardでSOTA

## QA

- マスクによって情報が失われているのでは？
  - そういうことも起こりうるが、人間が聞こえなくなったりすることも瞬間的にあるものの模擬だと思う
  - 前後のコンテキストを見てわかるようにさせてLocal情報に頼りすぎないようなモデルを作る。わざと少しいじめてやるという解釈

# Building the Singapore English National Speech Corpus (Interspeech)

## 柳田さん @NAIST D2、逐次音声合成の研究者

## 論文の背景

- シンガポール英語に興味があった
  - 英語だけじゃなく、マレー語、中国語、タミル語が混ざっている
  - 多言語の発音や語彙文法が混同し、Singlishが発生
  - 通常の英語認識タスクより困難になっている

## 提案手法

- 非ネイティブアクセントを含む大規模コーパスを構築
  - 音素バランス読み上げ文
  - 固有表現読み上げ文
- 性別分布は５０％
- 民族分布にも配慮
  - マレー系、インド系が相対的に少ないので、実際の民族構成よりも多めにとるようにした
- 単一音素、Biphone, Triphoneをバランスよく選択
- 人内の分散をカバーするため８回以上同じ文を読み上げてもらった
- SGの地名、住所、食べ物、著名人
- 語彙数：特に英語にない発音を重視した
  - 音素バランス：イギリス英語からの音素表記を修正
  - Local Word: 自動音素表記後に言語学者が修正
- 会話・自由発話コーパス
  - 2h x 250組 x 2条件の合計1000時間
  - あえて異なる民族同士の発話を録ったりもしている
- 言語切り替えがあったりもしているらしい（英語から中国語など）

## まとめ

- より困難な音声処理タスクに挑戦する必要がある中で、コーパス作成がより重要になる
- 言語の切り替えが発生しているようなASRについても今後取り組まれていくだろう
- National Singapore Corpusという名前で公開中



------

# SANTLR: Speech Annotation Toolkit for Low Resource Languages 

## 高道さん（猿渡研）

## 背景

- Rich resourceからlow resourceへ
- 音声言語処理の高精度化から、あらゆる言語をカバーし始めている動き
- 希少な言語に向けたプロジェクト
  - UNESCOが先住民族言語年に制定→消滅言語を保存していく呼びかけをした
  - DARPAのLORELEIというプロジェクトも
  - 科研費：日本語方言コーパスの構築（COJAD）
  - 科研費：多言語音声合成のための地理情報を利用した音韻・アクセントのモデリング
  - 



## 関連研究

- 統計モデルベース
  - Rich-resource languageからの転移学習
  - 音声言語規則の教師なし推定
- 音声言語資源の収集
  - Wikipediaの対訳テキスト
  - CMUの大規模コーパス Wilderness Multilingual Speech dataset
    - 聖書の音声データを利用
  - アノテーション技術
    - SPICE(2007)
    - SANTLR(本発表) -> Show&Tellというでもセッションでの発表

## 提案法

- 概要
  - WebベースのアノテーションSツール
  - 希少言語の収集やアノテーションできる
- Segmentation
- Transcribe

### UI

- 研究者とアノテーター両方にとって簡単なUIを用意した
- 全自動の前処理
  - HTMLタグや絵文字を自動消去
  - VADにより長い発話を自動分割する
  - 前処理後に共有可能な固有リンクを作成
- 研究者とAnnotator間で進捗共有できる

### Utterance Ranking

- 今までのアノテーションツールは対象音声を順々にアノテーションしていた
- しかし、実際には優先度が存在する（音響モデルに有効な音声を優先すべき）
- ２つのランキング機能を搭載している
  - Audio Ranking
    - Sort by duration
      - 発話の短い音声ほどやりやすい
      - 発話長でソートする（VADが入っているかどうかは不明）
    - SNR Calculation
    - Phoneme Overlaps
      - 被っている音素は優先度を下げる
      - 英語で言う「Yeah」「No」など重複が多いものはランクを落とす
  - Text Ranking
    - Perplexityでソート
      - 希少単語よりも頻出単語の方が発話しやすい->それを優先的に読むようにソートしている
    - Text Overlap
      - Phonemeと同じ発想

### 実験

- マイナー言語で収録。リテラシーのない人にはまだまだユーザガイドが必要である



## まとめ

- 音声収録、アノテーションを優しいUIで
- アノテーション優先度の計算



------

# Direct-Path Signal Cross-Correlation Estimation for Sound Source Localization in Reverberation (Interspeech)

## 山岡さん（首都大）

- D1, 小野研究室
- Mでは筑波大の牧野研究室
- 劣決定音声強調の研究に従事している
- ステレオマイクを用いた音声強調
- cos関数に関する２次補助関数

## 概要

- 残響がある中での音源定位をするときに相互相関を使う

## 背景

- DOA推定の時に、マイクアレイの時間差を利用して音源方向や位置を決定
- 位置情報サービスや、レーダー、ソナー、反射地震学でもやられている
- 難しさ、時間差を求める中で雑音や残響の影響が乗ってくる
- 音の伝搬の様子
- 数十msで波面が見えなくなるくらい雑音や残響が載る

## 従来法

- 観測信号からDOA推定値へのマッピング
  - 空間情報は一期一会（部屋の形状やマイク配置、残響）
  - どこでも使える汎用モデルの学習は困難
  - Chakrabarty, WASPAA 2017
  - W.Zhang, Inteerpspeech2019
- 信号処理ベース
  - 相互相関にもとづく
    - GCC-PHAT, GCC-SCOT, SRP-PHAT,
    - Parabolic interpolation、Zero padding
      - 残響だけを取り除けば結構タスクは簡単になるß
  - 信号部分空間：Music
  - 空間フィルタベース方法：ICAなど

## 関連研究

- （類似論文）残響除去による前処理：Nakatani, Kinoshita, Inteespeech2019
- 直接音に対する相互相関を推定したい
  - 残響成分が薄い周波数領域を使う手法
    - S.Mohan 2008
    - Moore+, 2015

## 提案法

- m番目のマイクにおける観測信号のSTFT表現
  - Y=H*S
  - 窓長がインパルス応答よりも短い場合、1フレーム前、２フレーム前、…の畳み込みで表現される
    - 窓長が十分短い場合、直接音は x = H_m,0 S(t,f)
    - 目的：i番目とj番目の相互相関を推定する
- 残響成分の推定
  - MCLPによる推定 [M. Delcloix+, 2007]
    - チャネル方向も活用した推定をする
- 直接音は、単純に残響成分を引き算すればいい
- DPCCの推定は、STFTしたら積になるのでそれを求めてからIFFTする
- 定位について
  - MCLP係数を推定
  - DPCCを推定
  - 従来の相互相関ベースの定位をする
    - 通常の相互相関の代わりにDPCCを使う

## 実験結果

- SNRが20-25dBと結構高い
- T60=400msとそんなに長くない
- 従来のSRP-PHATと比べて性能向上

## まとめ

- 残響に頑健な音源定位のための相互相関推定法を提案
- 従来法での相互相関をDPCCに置き換えるだけ
- Adaptiveなバージョンも提案されている
- 感想
  - T60=1sくらいでやってほしい
  - MCLP

------

# Unsupervised training of neural mask-based beamforming (Interspeech)

##  升山さん（早稲田大学）

- 表現工学科、産総研RA
- 位相を考慮した音響信号処理
- 多チャネル音源信号処理

## 背景

- 他チャネル音声強調BFをやりたい
- スマートスピーカなど遠いところから話しかけたい
- 目的音、妨害音の空間相関行列を推定したい
- 従来の教師あり手法
  - 実ペアデータの準備はコストが高い（クリーンなデータとノイジーな音声）
  - 多くの研究がシミュレーションデータを利用
    - ロンバート効果などのシミュレーションは困難
    - 拡散性の雑音のシミュレーションんは
  - Heymann+ 2016, DNNを用いたマスクベースのBFの初期の論文
    - モノラルのクリーン音声と雑音を学習に利用
    - バイナリマスクをターゲットとしてDNN学習

## 提案法

- 教師なしでの確率尤度の最大化を行う
- マスクベースのBF
  - 時間周波数マスクを推定
  - 空間相関行列を計算
  - BF（GEVビームフォーマーなど）
  - 観測信号をy, マスク推定をM, 空間相関行列がR, 右肩が音源のインデックス
- 各ブロックの実装
  - SCM
    - R = ΣMyy/ΣM
  - BFの設計
    - MVDR
    - GEV
- 直接cACGMMの尤度を最大化する
  - EMアルゴリズム位の結果が良くなるようにDNNを学習させる
  - cACGMMのEMアルゴリズムのM(+E)ステップを１回計算し、
  - 尤度に基づいて学習させる
    - cACGMMのパラメタπ、Bをマスクから計算する
    - 対数尤度が計算できるので、最大化する

## 関連研究

- End-to-end学習
  - Beamnet
- 擬似教師あり学習
  - Unsupevised Deep Clustering
    - 教師データを信号処理で作成する
    - 多チャネル混合音に従来のBSS手法を使う
- cACGMM[Ito+2016]: チャネル間の位相差・振幅差に注目
  - 空間的な性質の基づいて時間周波数マスクを生成
- DNNの学習
  - [Hershey+ 2016] たんチャネルの話者分離手法は低域や高域でうまくいきにくい
  - 学習には理想バイナリマスクを利用していた
    - **cACGMMのクラス割り当ての事後確率で代用する**

## 実験

- CHiME4データセット
- DNN
  - マスクEstimator: BiLSTM+Dense*3
- ロスの比較
  - クラス割り当ての事前確率が等しいという仮定を置いた場合に最高性能
  - EMアルゴリズムを追加すると改善
- 他手法との比較
  - 未処理：WER16%
  - cACGMM: 13%
  - 教師ありと遜色ないWERを教師なしで実現できる（クリーンデータのみでやる）
    - Oracle:7.71%
    - 教師あり：7.86%
    - 教師なし：7.80%

## まとめ

- マスク推定DNNの教師なし学習
- 確率モデルの尤度最大化による直接DNN学習
- 追加のEMステップ適用で教師ありの同程度の性能を獲得
- 課題：学習時にDNNの出力マスクに周波数方向のパーミュテーション問題が発生
- 発展
  - DNNで推定[Bando+2019]
  - 産総研のABCIは20000発話を2分で処理できる
